{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Documentazione API OpenAI\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In questa documentazione vedremo come utilizzare le API di OpenAI nel linguaggio python con l'utilizzo di Redis come database a vettori."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per prima dobbiamo sapere cos'è un database vettoriale e qual è il suo scopo: \n",
    "I database vettoriali sono database specializzati che hanno lo scopo di archiviare e gestire enormi quantità di dati ad alta dimensione sotto forma di vettori.\\\n",
    "I vettori sono rappresentazioni di dati matematici che descrivono oggetti in base alle loro diverse caratteristiche o qualità.\n",
    "In questa documetnazione come database vettoriale andremo ad utilizzare Redis (Remote Dictionary Server) che permette attraverso il salvataggio dei dati nella memoria cache delle prestazioni migliori [per approfondire Redis](https://redis.io/)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Redis girerà in un container (un container è un’unità di software che racchiude l’applicazione e tutte le sue dipendenze, isolandola dal sistema operativo e dall’hardware sottostante).\\\n",
    "Per creare il container utilizzeremo un altrò software chiamato Docker."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per prima cosa andiamo ad impostare il nostro Redis:\n",
    "- Creiamo un cartella chiamata *redis.conf*.\n",
    "- Creiamo un file *docker-compose.yml*.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All'interno del file *docker-compose.yml* inseriamo:\n",
    "\n",
    "\n",
    "```yaml\n",
    "version: '3.7'\n",
    "services:\n",
    "\n",
    "  vector-db:\n",
    "    image: redis/redis-stack:latest\n",
    "    ports:\n",
    "      - 6379:6379\n",
    "      - 8001:8001\n",
    "    environment:\n",
    "      - REDISEARCH_ARGS=CONCURRENT_WRITE_MODE\n",
    "    volumes:\n",
    "      - vector-db:/var/lib/redis\n",
    "      - ./redis.conf:/usr/local/etc/redis/redis.conf\n",
    "    healthcheck:\n",
    "      test: [\"CMD\", \"redis-cli\", \"-h\", \"localhost\", \"-p\", \"6379\", \"ping\"]\n",
    "      interval: 2s\n",
    "      timeout: 1m30s\n",
    "      retries: 5\n",
    "      start_period: 5s\n",
    "\n",
    "volumes:\n",
    "  vector-db:\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fatto questo, quando andremo ad avviare il nostro programma basterà fare `docker compose up` per far partire Redis.\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per utilizzare le API di OpenAI bisogna prendere la KEY che è creabile a questo [link](https://platform.openai.com/account/api-keys).\\\n",
    "Adesso andiamo a creare un file *.env* e al suo interno scriviamo creiamo una variabile d'ambiente:\\\n",
    "`OPENAI_API_KEY = \"OPENAI_API_KEY\"`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per prima cosa andiamo ad installare tutte le librerie che utilizzeremo:\n",
    "- `pip install openai`\n",
    "- `pip install redis`\n",
    "- `pip install wget`\n",
    "- `pip install pandas`\n",
    "- `pip install fire`\n",
    "- `pip install pypdf`\n",
    "- `pip install python-dotenv`\n",
    "\n",
    "O anche semplicemente:\n",
    "\n",
    "- `pip install openai redis wget pandas fire pypdf python-dotenv`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Andiamo quindi ad importare le nostre librerie:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import openai\n",
    "from pypdf import PdfReader\n",
    "from redis.commands.search.field import TextField, VectorField\n",
    "from redis.commands.search.indexDefinition import IndexDefinition, IndexType\n",
    "from redis.commands.search.query import Query\n",
    "import redis\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importiamo la nostra variabile d'ambiente contenuta nel file *.env*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Andiamo a definire i parametri per connetterci a Redis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INDEX_NAME = \"embeddings-index\"           # name of the search index\n",
    "PREFIX = \"doc\"                            # prefix for the document keys\n",
    "# distance metric for the vectors (ex. COSINE, IP, L2)\n",
    "DISTANCE_METRIC = \"COSINE\"\n",
    "\n",
    "REDIS_HOST = \"localhost\"\n",
    "REDIS_PORT = 6379\n",
    "REDIS_PASSWORD = \"\" \n",
    "\n",
    "redis_client = redis.Redis(\n",
    "            host=REDIS_HOST,\n",
    "            port=REDIS_PORT,\n",
    "            password=REDIS_PASSWORD\n",
    "        )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adesso creiamo le funzioni che utilizzeremo nel nostro programma:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questa funzione prende il *file.pdf* che gli viene passato in input, e restituisce una lista di dizionari aventi:\n",
    "- Id\n",
    "---\n",
    "- Embredding: è la rappresentazione numerica di un testo e serve per misurare la correlazione delle stringhe di testo\n",
    "---\n",
    "- Text: piccole porzioni di testo prese dal file in input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf_to_embeddings(pdf_path: str, chunk_length: int = 550):\n",
    "        # Read data from pdf file and split it into chunks\n",
    "        reader = PdfReader(pdf_path)\n",
    "        chunks = []\n",
    "        for page in reader.pages:\n",
    "            text_page = page.extract_text()\n",
    "            chunks.extend([text_page[i:i+chunk_length].replace('\\n', '')\n",
    "                          for i in range(0, len(text_page), chunk_length)])\n",
    "\n",
    "        # Create embeddings\n",
    "        response = openai.Embedding.create(\n",
    "            model='text-embedding-ada-002', input=chunks)\n",
    "        return [{'id': value['index'], 'vector':value['embedding'], 'text':chunks[value['index']]} for value in response['data']]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questa funzione ritorna una lista di dizionari come per la funzione che rigurda i *pdf* ma il testo in input è un *file.txt*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def txt_to_embeddings(text, chunk_length: int = 250):\n",
    "        # Read data from pdf file and split it into chunks\n",
    "        text = open(text, \"r\")\n",
    "        text = text.read()\n",
    "\n",
    "        chunks = []\n",
    "        \n",
    "        chunks.extend([text[i:i+chunk_length].replace('\\n', '')\n",
    "                       for i in range(0, len(text), chunk_length)])\n",
    "\n",
    "        # Create embeddings\n",
    "        response = openai.Embedding.create(\n",
    "            model='text-embedding-ada-002', input=chunks)\n",
    "        return [{'id': value['index'], 'vector':value['embedding'], 'text':chunks[value['index']]} for value in response['data']]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Facciamo lo stesso passando in input un *url*, per farlo dobbiamo però installare ed importare nuove librerie:\n",
    "- `pip install beautifulsoup4`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "def url_to_embeddings(url, chunk_length: int = 250):\n",
    "        # Read data from pdf file and split it into chunks\n",
    "\n",
    "        def format_url(testo):\n",
    "            pattern = r\"<.*?>\"  # Pattern per cercare \"<\" seguito da qualsiasi carattere, incluso il newline, fino a \">\"\n",
    "            testo_senza_angolari = re.sub(pattern, \"\", testo)  # Rimuovi i match del pattern dal testo\n",
    "            return testo_senza_angolari\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        page = requests.get(url)\n",
    "        soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "        body = soup.find('body')\n",
    "        content = str(soup.find_all(\"p\"))\n",
    "    \n",
    "        text = format_url(content)\n",
    "\n",
    "\n",
    "        chunks = []\n",
    "        \n",
    "        chunks.extend([text[i:i+chunk_length].replace('\\n', '')\n",
    "                       for i in range(0, len(text), chunk_length)])\n",
    "\n",
    "        # Create embeddings\n",
    "        response = openai.Embedding.create(\n",
    "            model='text-embedding-ada-002', input=chunks)\n",
    "        return [{'id': value['index'], 'vector':value['embedding'], 'text':chunks[value['index']]} for value in response['data']]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prima di caricare il testo su Redis assicuriamoci che siano già stati forniti altri testi, nel caso ci fossero altri dati andiamo a cancellarli:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_redis_data(index_name: str = INDEX_NAME):\n",
    "        try:\n",
    "            redis_client.flushdb()\n",
    "            print('Index dropped')\n",
    "        except:\n",
    "            # Index doees not exist\n",
    "            print('Index does not exist')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fatto questo carichiamo la lista di dizionari su Redis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_to_redis(embeddings):\n",
    "        # Constants\n",
    "        vector_dim = len(embeddings[0]['vector'])  # length of the vectors\n",
    "        \n",
    "\t\t# Initial number of vectors\n",
    "        vector_number = len(embeddings)\n",
    "\n",
    "        # Define RediSearch fields\n",
    "        text = TextField(name=\"text\")\n",
    "        text_embedding = VectorField(\"vector\",\n",
    "                                     \"FLAT\", {\n",
    "                                         \"TYPE\": \"FLOAT32\",\n",
    "                                         \"DIM\": vector_dim,\n",
    "                                         \"DISTANCE_METRIC\": \"COSINE\",\n",
    "                                         \"INITIAL_CAP\": vector_number,\n",
    "                                     }\n",
    "                                     )\n",
    "        fields = [text, text_embedding]\n",
    "\n",
    "        # Check if index exists\n",
    "        try:\n",
    "            redis_client.ft(INDEX_NAME).info()\n",
    "            print(\"Index already exists\")\n",
    "        except:\n",
    "            # Create RediSearch Index\n",
    "            redis_client.ft(INDEX_NAME).create_index(\n",
    "                fields=fields,\n",
    "                definition=IndexDefinition(\n",
    "                    prefix=[PREFIX], index_type=IndexType.HASH)\n",
    "            )\n",
    "\n",
    "        for embedding in embeddings:\n",
    "            key = f\"{PREFIX}:{str(embedding['id'])}\"\n",
    "            embedding[\"vector\"] = np.array(\n",
    "                embedding[\"vector\"], dtype=np.float32).tobytes()\n",
    "            redis_client.hset(key, mapping=embedding)\n",
    "        print(\n",
    "            f\"Loaded {redis_client.info()['db0']['keys']} documents in Redis search index with name: {INDEX_NAME}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una volta salvati i dati dobbiamo estrarre le parole chiavi dalla domanda dell'utente, per poi successivamente fare l'embedding di quest'ultime.\\\n",
    "Per farlo utilizziamo il modello *gpt-3.5-turbo* di OpenAI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_intent(user_question: str):\n",
    "         # call the openai ChatCompletion endpoint\n",
    "         response = openai.ChatCompletion.create(\n",
    "         model=\"gpt-3.5-turbo\",\n",
    "         messages=[\n",
    "               {\"role\": \"user\", \"content\": f'Extract the keywords from the following question: {user_question}'+\n",
    "                 'Do not answer anything else, only the keywords.'}\n",
    "            ]\n",
    "         )\n",
    "         \n",
    "\n",
    "         # extract the response\n",
    "         return (response['choices'][0]['message']['content'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prese le parole chiave dela domanda, facciamo l'embedding di quest'ultime e le confrontiamo con i nostri dati presenti in Redis.\\\n",
    "Facendo questo saremo in grado di prendere la porzione di testo nella quale è presente la risposta alla domanda:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_redis(user_query: str,\n",
    "                     index_name: str = \"embeddings-index\",\n",
    "                     vector_field: str = \"vector\",\n",
    "                     return_fields: list = [\"text\", \"vector_score\"],\n",
    "                     hybrid_fields=\"*\",\n",
    "                     k: int = 20,\n",
    "                     print_results: bool = False,\n",
    "                     ):\n",
    "        # Creates embedding vector from user query\n",
    "        embedded_query = openai.Embedding.create(input=user_query,\n",
    "                                                 model=\"text-embedding-ada-002\",\n",
    "                                                 )[\"data\"][0]['embedding']\n",
    "        # Prepare the Query\n",
    "        base_query = f'{hybrid_fields}=>[KNN {k} @{vector_field} $vector AS vector_score]'\n",
    "        query = (\n",
    "            Query(base_query)\n",
    "            .return_fields(*return_fields)\n",
    "            .sort_by(\"vector_score\")\n",
    "            .paging(0, k)\n",
    "            .dialect(2)\n",
    "        )\n",
    "        params_dict = {\"vector\": np.array(\n",
    "            embedded_query).astype(dtype=np.float32).tobytes()}\n",
    "        # perform vector search\n",
    "        results = self.redis_client.ft(index_name).search(query, params_dict)\n",
    "        if print_results:\n",
    "            for i, doc in enumerate(results.docs):\n",
    "                score = 1 - float(doc.vector_score)\n",
    "                print(f\"{i}. {doc.text} (Score: {round(score ,3) })\")\n",
    "        return [doc['text'] for doc in results.docs]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per avere la risposta alla nostra domanda passiamo al modello *gpt-3.5-turbo* la nostra domanda e il testo dalla quale prendere le informazioni per rispondere:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(facts, user_question):\n",
    "         # call the openai ChatCompletion endpoint\n",
    "         response = openai.ChatCompletion.create(\n",
    "         model=\"gpt-3.5-turbo\",\n",
    "         messages=[\n",
    "               {\"role\": \"user\", \"content\": 'Based on the FACTS, give an answer to the QUESTION.'+ \n",
    "                f'QUESTION: {user_question}. FACTS: {facts}'}\n",
    "            ]\n",
    "         )\n",
    "\n",
    "         \n",
    "\n",
    "         # extract the response\n",
    "         return (response['choices'][0]['message']['content'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testiamo il nostro programma"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PDF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example pdf\n",
    "pdf = 'giochi.pdf'\n",
    "\n",
    "\n",
    "# Drop all data from redis if needed\n",
    "data_service.drop_redis_data()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Load data from pdf to redis\n",
    "data = data_service.pdf_to_embeddings(pdf)\n",
    "\n",
    "data_service.load_data_to_redis(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
