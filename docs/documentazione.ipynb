{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Documentazione API OpenAI\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In questa documentazione vedremo come utilizzare le API di OpenAI nel linguaggio python con l'utilizzo di Redis come database a vettori."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prima di incominciare dobbiamo sapere cos'è un database vettoriale e qual è il suo scopo:\\\n",
    "I database vettoriali sono database specializzati che hanno lo scopo di archiviare e gestire enormi quantità di dati ad alta dimensione sotto forma di vettori.\\\n",
    "I vettori sono rappresentazioni di dati matematici che descrivono oggetti in base alle loro diverse caratteristiche o qualità.\n",
    "In questa documetnazione come database vettoriale andremo ad utilizzare Redis (Remote Dictionary Server) che permette attraverso il salvataggio dei dati nella memoria cache delle prestazioni migliori [per approfondire Redis](https://redis.io/)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Redis girerà in un container (un container è un’unità di software che racchiude l’applicazione e tutte le sue dipendenze, isolandola dal sistema operativo e dall’hardware sottostante).\\\n",
    "Per creare il container utilizzeremo un altrò software chiamato Docker."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per prima cosa andiamo ad impostare il nostro Redis:\n",
    "- Creiamo un cartella chiamata *redis.conf*.\n",
    "- Creiamo un file *docker-compose.yml*.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All'interno del file *docker-compose.yml* inseriamo:\n",
    "\n",
    "\n",
    "```yaml\n",
    "version: '3.7'\n",
    "services:\n",
    "\n",
    "  vector-db:\n",
    "    image: redis/redis-stack:latest\n",
    "    ports:\n",
    "      - 6379:6379\n",
    "      - 8001:8001\n",
    "    environment:\n",
    "      - REDISEARCH_ARGS=CONCURRENT_WRITE_MODE\n",
    "    volumes:\n",
    "      - vector-db:/var/lib/redis\n",
    "      - ./redis.conf:/usr/local/etc/redis/redis.conf\n",
    "    healthcheck:\n",
    "      test: [\"CMD\", \"redis-cli\", \"-h\", \"localhost\", \"-p\", \"6379\", \"ping\"]\n",
    "      interval: 2s\n",
    "      timeout: 1m30s\n",
    "      retries: 5\n",
    "      start_period: 5s\n",
    "\n",
    "volumes:\n",
    "  vector-db:\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fatto questo, quando andremo ad avviare il nostro programma basterà fare `docker compose up` per far partire Redis.\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per utilizzare le API di OpenAI bisogna prendere la KEY che è creabile a questo [link](https://platform.openai.com/account/api-keys).\\\n",
    "Adesso andiamo a creare un file *.env* e al suo interno scriviamo creiamo una variabile d'ambiente:\\\n",
    "`OPENAI_API_KEY = \"OPENAI_API_KEY\"`\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Andiamo a scaricare ed importare le nostre librerie:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per prima cosa andiamo ad installare tutte le librerie che utilizzeremo:\n",
    "- `pip install openai`\n",
    "- `pip install redis`\n",
    "- `pip install wget`\n",
    "- `pip install pandas`\n",
    "- `pip install fire`\n",
    "- `pip install pypdf`\n",
    "- `pip install python-dotenv`\n",
    "\n",
    "O anche semplicemente:\n",
    "\n",
    "- `pip install openai redis wget pandas fire pypdf python-dotenv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importiamo le librerie\n",
    "import os\n",
    "import numpy as np\n",
    "import openai\n",
    "from pypdf import PdfReader\n",
    "from redis.commands.search.field import TextField, VectorField\n",
    "from redis.commands.search.indexDefinition import IndexDefinition, IndexType\n",
    "from redis.commands.search.query import Query\n",
    "import redis\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importiamo la nostra variabile d'ambiente contenuta nel file *.env*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "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\n"
     ]
    }
   ],
   "source": [
    "# Carichiamo il file .Env\n",
    "load_dotenv()\n",
    "# Assegnamo la API KEY\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "print(openai.api_key)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Andiamo a definire i parametri e ci connettiamo a Redis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "INDEX_NAME = \"embeddings-index\"           # Definiamo il nome dell'indice di ricerca\n",
    "PREFIX = \"doc\"                            # Definiamo un prefisso per le chiavi dei documenti\n",
    "DISTANCE_METRIC = \"COSINE\" # Definiamo la metrica di distanza utilizzata per i vettori (ex. COSINE, IP, L2)\n",
    "\n",
    "# Definiamo le variabili per la connessione a Redis\n",
    "REDIS_HOST = \"localhost\"\n",
    "REDIS_PORT = 6379\n",
    "REDIS_PASSWORD = \"\" \n",
    "\n",
    "# Creiamo un istanza del client Redis\n",
    "redis_client = redis.Redis(\n",
    "            host=REDIS_HOST,\n",
    "            port=REDIS_PORT,\n",
    "            password=REDIS_PASSWORD\n",
    "        )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adesso creiamo le funzioni che utilizzeremo nel nostro programma:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questa funzione prende il *file.pdf* che gli viene passato in input, e restituisce una lista di dizionari aventi:\n",
    "- Id\n",
    "---\n",
    "- Embedding: è la rappresentazione numerica di un testo e serve per misurare la correlazione delle stringhe di testo\n",
    "---\n",
    "- Text: piccole porzioni di testo prese dal file in input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definiamo la funzione pdf_to_embeddings (percorso del pdf,  lunghezza dei frammenti in cui il testo del PDF verrà suddiviso)\n",
    "def pdf_to_embeddings(pdf_path: str, chunk_length: int = 550):\n",
    "        # Leggiamo il file pdf e lo suddividiamo in chunks (frammenti più piccoli)\n",
    "        reader = PdfReader(pdf_path)\n",
    "        chunks = []\n",
    "\n",
    "        # Facciamo un ciclo for che estragga il testo da ogni pagina del pdf e vada a suddividerlo in chunkks\n",
    "        for page in reader.pages:\n",
    "            text_page = page.extract_text()\n",
    "            chunks.extend([text_page[i:i+chunk_length].replace('\\n', '')\n",
    "                          for i in range(0, len(text_page), chunk_length)])\n",
    "\n",
    "        # Facciamo l'embedding dei chunks\n",
    "        response = openai.Embedding.create(\n",
    "            model='text-embedding-ada-002', input=chunks)\n",
    "        return [{'id': value['index'], 'vector':value['embedding'], 'text':chunks[value['index']]} for value in response['data']]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questa funzione ritorna una lista di dizionari come per la funzione che rigurda i *pdf* ma il testo in input è un *file.txt*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definiamo la funzione txt_to_embeddings (percorso del file txt,  lunghezza dei frammenti in cui il testo del txt verrà suddiviso)\n",
    "def txt_to_embeddings(text, chunk_length: int = 250):\n",
    "        # Leggiamo il file txt e lo suddividiamo in chunks (frammenti più piccoli)\n",
    "        text = open(text, \"r\")\n",
    "        text = text.read()\n",
    "\n",
    "        chunks = []\n",
    "        \n",
    "        # Suddividiamo il testo in chunk \n",
    "        chunks.extend([text[i:i+chunk_length].replace('\\n', '')\n",
    "                       for i in range(0, len(text), chunk_length)])\n",
    "\n",
    "        # Facciamo l'embedding dei chunks\n",
    "        response = openai.Embedding.create(\n",
    "            model='text-embedding-ada-002', input=chunks)\n",
    "        return [{'id': value['index'], 'vector':value['embedding'], 'text':chunks[value['index']]} for value in response['data']]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Facciamo lo stesso passando in input un *url*, per farlo dobbiamo però installare ed importare nuove librerie:\n",
    "- `pip install beautifulsoup4`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importiamo le librerie utili per fare Web Screaping\n",
    "import requests \n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "# Definiamo la funzione url_to_embeddings (indirizzo URL,  lunghezza dei frammenti in cui il testo della pagina web verrà suddiviso)\n",
    "def url_to_embeddings(url, chunk_length: int = 250):\n",
    "\n",
    "        # Definiamo una funzione format_url che formatti il testo estrapolato dalla pagina HTML, anando ad eliminare simboli comuni (es. \"<>\")\n",
    "        def format_url(testo):\n",
    "            pattern = r\"<.*?>\"  # Pattern per cercare \"<\" seguito da qualsiasi carattere, incluso il newline, fino a \">\"\n",
    "            testo_senza_angolari = re.sub(pattern, \"\", testo)  # Rimuovi i match del pattern dal testo\n",
    "            return testo_senza_angolari\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        page = requests.get(url) # Otteniamo il contenuto della pagina web\n",
    "\n",
    "        # Analizziamo ed estrapoliamo le informazione che ci servono dal file HTML \n",
    "        soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "        body = soup.find('body')\n",
    "        content = str(soup.find_all(\"p\"))\n",
    "\n",
    "        # Richiamiamo la funzione per formattare il testo estrapolato\n",
    "        text = format_url(content)\n",
    "\n",
    "\n",
    "        chunks = []\n",
    "\n",
    "        # Suddividiamo il testo in chunk \n",
    "        chunks.extend([text[i:i+chunk_length].replace('\\n', '')\n",
    "                       for i in range(0, len(text), chunk_length)])\n",
    "\n",
    "        # Creiamo l'embedding dei chunks\n",
    "        response = openai.Embedding.create(\n",
    "            model='text-embedding-ada-002', input=chunks)\n",
    "        return [{'id': value['index'], 'vector':value['embedding'], 'text':chunks[value['index']]} for value in response['data']]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prima di caricare il testo su Redis assicuriamoci che siano già stati forniti altri testi, nel caso ci fossero altri dati andiamo a cancellarli:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definiamo la funzione drop_redis_data (nome dell'indice di ricerca)\n",
    "def drop_redis_data(index_name: str = INDEX_NAME):\n",
    "    # Controlliamo che sono presenti dei dati nel db \n",
    "    if redis_client.dbsize() > 0:\n",
    "        redis_client.flushdb() # Eliminiamo tutti i dati presenti nel db Redis\n",
    "        print('Index dropped')\n",
    "    else:\n",
    "        print('Index does not exist')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fatto questo carichiamo la lista di dizionari su Redis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definiamo la funzione load_data_to_redis (lista di dizionari ricevuta dopo aver fatto l'embedding di uun testo)\n",
    "def load_data_to_redis(embeddings):\n",
    "\n",
    "        vector_dim = len(embeddings[0]['vector'])  # Lunghezza dei vettori\n",
    "        \n",
    "\t\t# Numero iniziale di vettori\n",
    "        vector_number = len(embeddings)\n",
    "\n",
    "        # Definizione dei campi di RediSearch\n",
    "        text = TextField(name=\"text\")\n",
    "        text_embedding = VectorField(\"vector\",\n",
    "                                     \"FLAT\", {\n",
    "                                         \"TYPE\": \"FLOAT32\",\n",
    "                                         \"DIM\": vector_dim,\n",
    "                                         \"DISTANCE_METRIC\": \"COSINE\",\n",
    "                                         \"INITIAL_CAP\": vector_number,\n",
    "                                     }\n",
    "                                     )\n",
    "        fields = [text, text_embedding]\n",
    "\n",
    "        # Verifica se l'indice esiste già\n",
    "        try:\n",
    "            redis_client.ft(INDEX_NAME).info()\n",
    "            print(\"Index already exists\")\n",
    "        except:\n",
    "            # Creazione dell'indice RediSearch\n",
    "            redis_client.ft(INDEX_NAME).create_index(\n",
    "                fields=fields,\n",
    "                definition=IndexDefinition(\n",
    "                    prefix=[PREFIX], index_type=IndexType.HASH)\n",
    "            )\n",
    "\n",
    "        # Viene iterato su ciascun elemento in embeddings\n",
    "        for embedding in embeddings:\n",
    "            key = f\"{PREFIX}:{str(embedding['id'])}\" # Viene generata una chiave key utilizzando l'ID dell'embedding e il prefisso specificato\n",
    "            # Il vettore di embedding viene convertito in un array di tipo np.float32\n",
    "            embedding[\"vector\"] = np.array(\n",
    "                embedding[\"vector\"], dtype=np.float32).tobytes() # Poi convertito in formato di byte utilizzando tobytes() \n",
    "            redis_client.hset(key, mapping=embedding) # Salviamo l'embedding nella chiave Redis corrispondente\n",
    "        \n",
    "        # Stampiamo un messaggio che indica il numero di documenti caricati nell'indice di ricerca Redis con il nome specificato.\n",
    "        print(\n",
    "            f\"Loaded {redis_client.info()['db0']['keys']} documents in Redis search index with name: {INDEX_NAME}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una volta salvati i dati dobbiamo estrarre le parole chiavi dalla domanda dell'utente, per poi successivamente fare l'embedding di quest'ultime.\\\n",
    "Per farlo utilizziamo il modello *gpt-3.5-turbo* di OpenAI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definiamo la funzione get_intent (domanda posta dall'utente)\n",
    "def get_intent(user_question: str):\n",
    "         # Chiamiamo l'endpoint openai ChatCompletion \n",
    "         response = openai.ChatCompletion.create(\n",
    "         model=\"gpt-3.5-turbo\", # Indichiamo il modello da utilzzare\n",
    "\n",
    "         # Chiediamo di estrapolare le parole chiave dalla domanda dell'utente\n",
    "         messages=[\n",
    "               {\"role\": \"user\", \"content\": f'Extract the keywords from the following question: {user_question}'+\n",
    "                 'Do not answer anything else, only the keywords.'}\n",
    "            ]\n",
    "         )\n",
    "         \n",
    "\n",
    "         # Estriamo la rispsota\n",
    "         return (response['choices'][0]['message']['content'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prese le parole chiave dela domanda, facciamo l'embedding di quest'ultime e le confrontiamo con i nostri dati presenti in Redis.\\\n",
    "Facendo questo saremo in grado di prendere la porzione di testo nella quale è presente la risposta alla domanda:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definiamo la funzione search_redis \n",
    "def search_redis(user_query: str,\n",
    "                     index_name: str = \"embeddings-index\",\n",
    "                     vector_field: str = \"vector\",\n",
    "                     return_fields: list = [\"text\", \"vector_score\"],\n",
    "                     hybrid_fields=\"*\",\n",
    "                     k: int = 20,\n",
    "                     print_results: bool = False,\n",
    "                     ):\n",
    "\n",
    "        # Creiamo l'embedding della query dell'utente\n",
    "        embedded_query = openai.Embedding.create(input=user_query,\n",
    "                                                 model=\"text-embedding-ada-002\",\n",
    "                                                 )[\"data\"][0]['embedding']\n",
    "\n",
    "        # Prepariamo la query \n",
    "        '''\n",
    "        Viene definita la query di base base_query utilizzando la sintassi di ricerca RediSearch. La variabile hybrid_fields viene utilizzata per specificare \n",
    "        i campi da includere nella ricerca. Viene utilizzato l'operatore KNN per eseguire una ricerca K-nearest neighbor sull'indice utilizzando il campo \n",
    "        vettoriale specificato da vector_field. Il risultato della ricerca viene restituito con il nome vector_score.\n",
    "        '''\n",
    "        base_query = f'{hybrid_fields}=>[KNN {k} @{vector_field} $vector AS vector_score]'\n",
    "\n",
    "        '''\n",
    "        La query viene preparata utilizzando il modulo Query di RediSearch. Vengono specificati i campi da restituire con il metodo return_fields, \n",
    "        si ordina per vector_score utilizzando sort_by, si specifica la paginazione con paging e si imposta il dialetto a 2 con dialect.\n",
    "        '''\n",
    "        query = (\n",
    "            Query(base_query)\n",
    "            .return_fields(*return_fields)\n",
    "            .sort_by(\"vector_score\")\n",
    "            .paging(0, k)\n",
    "            .dialect(2)\n",
    "        )\n",
    "\n",
    "        # Convertiamo embedded_query\n",
    "        params_dict = {\"vector\": np.array(\n",
    "            embedded_query).astype(dtype=np.float32).tobytes()}\n",
    "\n",
    "        # Eseguiamo la ricerca vettoriale\n",
    "        results = redis_client.ft(index_name).search(query, params_dict)\n",
    "        if print_results:\n",
    "            for i, doc in enumerate(results.docs):\n",
    "                score = 1 - float(doc.vector_score)\n",
    "                print(f\"{i}. {doc.text} (Score: {round(score ,3) })\")\n",
    "        return [doc['text'] for doc in results.docs]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per avere la risposta alla nostra domanda passiamo al modello *gpt-3.5-turbo* la nostra domanda e il testo dalla quale prendere le informazioni per rispondere:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definiamo la funzione generate_response (informazioni dalla quale estrarre la risposta, domanda posta dall'utente)\n",
    "def generate_response(facts, user_question):\n",
    "         # Chiamiamo l'endpoint openai ChatCompletion \n",
    "         response = openai.ChatCompletion.create(\n",
    "         model=\"gpt-3.5-turbo\", # Indichiamo il modello da utilzzare\n",
    "         \n",
    "         # Chiediamo di rispondere alla domanda estrapolando le informazioni da facts\n",
    "         messages=[\n",
    "               {\"role\": \"user\", \"content\": 'Based on the FACTS, give an answer to the QUESTION.'+ \n",
    "                f'QUESTION: {user_question}. FACTS: {facts}'}\n",
    "            ]\n",
    "         )\n",
    "\n",
    "         \n",
    "\n",
    "         # Estraiamo la risposta\n",
    "         return (response['choices'][0]['message']['content'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testiamo il nostro programma"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### PDF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index dropped\n",
      "Loaded 75 documents in Redis search index with name: embeddings-index\n"
     ]
    }
   ],
   "source": [
    "# indichiamo il percorso del file pdf\n",
    "pdf = 'giochi.pdf'\n",
    "\n",
    "# Richiamiamo la funzione drop_redis_data per eliminiare i dati dal db se serve\n",
    "drop_redis_data()\n",
    "\n",
    "# Richiamiamo la funzione pdf_to_embeddings per creare l'embedding del file\n",
    "data = pdf_to_embeddings(pdf)\n",
    "\n",
    "# Richiamiamo la funzione load_data_to_redis per caricare data su Redis\n",
    "load_data_to_redis(data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poniamo una domanda che riguarda il file che ha caricato:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Il documento parla dell'argomento \"Videogames e Game Studies\", esplorando la storia, la teoria e l'evoluzione dei videogiochi dal punto di vista culturale, sociologico e narrativo. Viene inoltre discusso il ruolo del videogioco nella società e la tassonomia dei diversi tipi di giocatori. Il documento esplora anche l'evoluzione della narrazione nei videogiochi, dal semplice intrattenimento ai giochi che offrono un'esperienza narrativa completa. Infine, sono presenti conclusioni e fonti bibliografiche e informative per ulteriori approfondimenti.\n"
     ]
    }
   ],
   "source": [
    "# Poniamo una domanda rigurdante il file\n",
    "question = \"Di cosa parla il documento?\"\n",
    "\n",
    "# Richiamiamo la funzione get_intent per estrapolare le parole chiave dalla domanda \n",
    "intents = get_intent(question)\n",
    "\n",
    "# Richiamiamo la funzione search_redis per trovare le parti di testo inerenti alla domanda\n",
    "facts = search_redis(intents)\n",
    "\n",
    "# Richiamiamo la funzione generate_responde per ricevere la risposta alla domanda\n",
    "answer = generate_response(facts, question)\n",
    "\n",
    "# Stampiamo la risposta\n",
    "print(answer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TXT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index dropped\n",
      "Loaded 844 documents in Redis search index with name: embeddings-index\n"
     ]
    }
   ],
   "source": [
    "# indichiamo il percorso del file tct\n",
    "txt = '6.txt'\n",
    "\n",
    "# Richiamiamo la funzione drop_redis_data per eliminiare i dati dal db se serve\n",
    "drop_redis_data()\n",
    "\n",
    "# Richiamiamo la funzione txt_to_embeddings per creare l'embedding del file\n",
    "data = txt_to_embeddings(txt)\n",
    "\n",
    "# Richiamiamo la funzione load_data_to_redis per caricare data su Redis\n",
    "load_data_to_redis(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I fondatori di Apple sono Steve Jobs, Steve Wozniak, and Ronald Wayne.\n"
     ]
    }
   ],
   "source": [
    "# Poniamo una domanda rigurdante il file\n",
    "question = \"Chi sono i fondatori di Apple?\"\n",
    "\n",
    "# Richiamiamo la funzione get_intent per estrapolare le parole chiave dalla domanda \n",
    "intents = get_intent(question)\n",
    "\n",
    "# Richiamiamo la funzione search_redis per trovare le parti di testo inerenti alla domanda\n",
    "facts = search_redis(intents)\n",
    "\n",
    "# Richiamiamo la funzione generate_responde per ricevere la risposta alla domanda\n",
    "answer = generate_response(facts, question)\n",
    "\n",
    "# Stampiamo la risposta\n",
    "print(answer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### URL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index dropped\n",
      "Loaded 18 documents in Redis search index with name: embeddings-index\n"
     ]
    }
   ],
   "source": [
    "# Indichiamo l'URL della pagina sulla quale vogliamo fare porre delle domande\n",
    "url = 'https://www.ilpost.it/2023/06/05/apple-vision-pro/'\n",
    "\n",
    "# Richiamiamo la funzione drop_redis_data per eliminiare i dati dal db se serve\n",
    "drop_redis_data()\n",
    "\n",
    "# Richiamiamo la funzione url_to_embeddings per creare l'embedding del file\n",
    "data = url_to_embeddings(url)\n",
    "\n",
    "# Richiamiamo la funzione load_data_to_redis per caricare data su Redis\n",
    "load_data_to_redis(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caratteristiche principali dell'Apple Vision Pro includono il suo prezzo di $3,499, la possibilità di funzionare con alcuni accessori come una tastiera e un trackpad, il tracciamento del movimento degli occhi, la risoluzione a 4K dei suoi schermi, il fatto che non si può vedere attraverso di esso ma riprende ciò che si ha intorno grazie a un set di 12 fotocamere, e la sua capacità di funzionare sia in modalità realtà aumentata sia in modalità realtà virtuale. Tuttavia, ci sono ancora dubbi circa la sua autonomia e le sue applicazioni pratiche.\n"
     ]
    }
   ],
   "source": [
    "# Poniamo una domanda rigurdante il file\n",
    "question = \"Quali sono le caratteristiche principarli dell'Apple Vision Pro?\"\n",
    "\n",
    "# Richiamiamo la funzione get_intent per estrapolare le parole chiave dalla domanda \n",
    "intents = get_intent(question)\n",
    "\n",
    "# Richiamiamo la funzione search_redis per trovare le parti di testo inerenti alla domanda\n",
    "facts = search_redis(intents)\n",
    "\n",
    "# Richiamiamo la funzione generate_responde per ricevere la risposta alla domanda\n",
    "answer = generate_response(facts, question)\n",
    "\n",
    "# Stampiamo la risposta\n",
    "print(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
